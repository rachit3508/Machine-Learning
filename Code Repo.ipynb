{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Label encoding on 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "data['Attrition'] = enc.fit_transform(data['Attrition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding on 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "X=ohe.fit_transform(data['JobRole'].values.reshape(-1,1)).toarray()\n",
    "onehotdf = pd.DataFrame(X,columns=[i for i in data['JobRole'].unique()])\n",
    "onehotdf.drop(onehotdf.columns[0], axis=1,inplace=True)\n",
    "data = pd.concat([data, onehotdf], axis=1)\n",
    "data= data.drop(['JobRole'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding on Multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "for col in new_data.columns:\n",
    "    data[col] = enc.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding on multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "for col in new_data1.columns:\n",
    "    X=ohe.fit_transform(data[col].values.reshape(-1,1)).toarray()\n",
    "    onehotdf = pd.DataFrame(X,columns=[i for i in data[col].unique()])\n",
    "    onehotdf.drop(onehotdf.columns[0], axis=1,inplace=True)\n",
    "    data = pd.concat([data, onehotdf], axis=1)\n",
    "    data= data.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Accuracy for classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_train)\n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        pred = clf.predict(X_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100,1000], \n",
    "              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001,.0001,.00001], \n",
    "              'kernel': ['rbf','linear']} \n",
    "\n",
    "grid = GridSearchCV(SVC(),param_grid,scoring=make_scorer(f1_score), refit=True, verbose=1, cv=5,n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid.best_params_\n",
    "print(f\"Best params: {best_params}\")\n",
    "\n",
    "svm_clf = SVC(**best_params)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "print_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(svm_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting boxplot for all the columns and checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,3, figsize = (15,15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(0,len(data.columns)-1):\n",
    "    sb.boxplot(y=data.iloc[:,i], data=data, orient='v', ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Target variable into number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.Status=='Abnormal','Status'] = 1\n",
    "data.loc[data.Status=='Normal','Status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, threshholds = metrics.roc_curve(y_test,  y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"ROC curve , auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression(fit_intercept=True)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Coefficients of variables\n",
    "coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])\n",
    "coeff_df\n",
    "\n",
    "#intercept\n",
    "regressor.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the OLS assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether mean of error lies at 0\n",
    "plt.scatter(y_pred, (y_test-y_pred))\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "\n",
    "(y_test-y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_endog = sm.add_constant(X_train)\n",
    "X_endog1 = sm.add_constant(X_test)\n",
    "res = sm.OLS(y_train, X_endog)\n",
    "res.fit()\n",
    "res.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Score method for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = dataset.Paved_Highways.mean() + 3*dataset.Paved_Highways.std()\n",
    "lower_limit = dataset.Paved_Highways.mean() - 3*dataset.Paved_Highways.std()\n",
    "\n",
    "#checking the outliers\n",
    "dataset[(dataset['Paved_Highways']>upper_limit) | (dataset['Paved_Highways']<lower_limit)]\n",
    "\n",
    "#Removing the outliers\n",
    "new_data = data[(data['Paved_Highways']<upper_limit) & (data['Paved_Highways']>lower_limit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the interaction effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x_interaction = PolynomialFeatures(2,interaction_only=True,include_bias=False).fit_transform(x)\n",
    "interaction_df = pd.DataFrame(x_interaction,columns=['TV','Radio','Newspaper','TV:Radio','TV:newspaper','Radio:Newspaper'])\n",
    "from statsmodels.regression import linear_model\n",
    "interaction_model = linear_model.OLS(y,interaction_df).fit()\n",
    "interaction_model.pvalues[interaction_model.pvalues<0.05]\n",
    "\n",
    "\n",
    "data['TV:Radio'] = data['TV']*data['Radio']\n",
    "data['TV:Newspaper'] = data['TV']*data['Newspaper']\n",
    "data['Radio:Newspaper'] = data['Radio']*data['Newspaper']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing outliers using boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "sns.boxplot(data = creditcard_df,width=0.5,ax=ax,fliersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow method and sillhout method for selecting no of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(16, 9))\n",
    "elbow = figure.add_subplot(1,2,1) #elbow chart\n",
    "kmean_sil = figure.add_subplot(1,2,2) #silhouette bar chart\n",
    "\n",
    "n_clusters=19\n",
    "cost=[]\n",
    "for i in range(1,n_clusters):\n",
    "    kmean= KMeans(i)\n",
    "    kmean.fit(creditcard_df_scaled)\n",
    "    cost.append(kmean.inertia_) \n",
    "    elbow.set_ylabel('Sum of Squared Errors', fontsize = 15)\n",
    "    elbow.set_xlabel('Number of Clusters', fontsize = 15)\n",
    "    elbow.set_title('K-MEANS Clustering SSE: Elbow Chart', fontsize = 15)\n",
    "    elbow.plot(cost, 'bx-')\n",
    "\n",
    "\n",
    "silhouette_scores = [] \n",
    "\n",
    "for n_cluster in range(2, 20):\n",
    "    silhouette_scores.append( \n",
    "        silhouette_score(creditcard_df_scaled, KMeans(n_clusters = n_cluster).fit_predict(creditcard_df_scaled))) \n",
    "    \n",
    "# Plotting a bar graph to compare the results \n",
    "k = [2, 3, 4, 5, 6,7,8,9,10,11,12,13,14,15,16,17,18,19] \n",
    "kmean_sil.bar(k, silhouette_scores) \n",
    "kmean_sil.set_title('K-MEANS: Number of Clusters vs. Silhouette Score', fontsize = 15)\n",
    "kmean_sil.set_xlabel('Number of Clusters', fontsize = 15) \n",
    "kmean_sil.set_ylabel('Silhouette Score', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating the cluster to each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the KMeans model\n",
    "kmeans = KMeans(5,random_state=6)\n",
    "kmeans.fit(creditcard_df_scaled)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "creditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\n",
    "creditcard_df_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No of values in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=creditcard_df_cluster['cluster'], columns='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_train[df_train.Attrition==0]\n",
    "df_minority = df_train[df_train.Attrition==1]\n",
    "\n",
    "print(df_majority.Attrition.count())\n",
    "print(\"-----------\")\n",
    "print(df_minority.Attrition.count())\n",
    "print(\"-----------\")\n",
    "print(df_train.Attrition.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=600,    # to match majority class\n",
    "                                 random_state=10) # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "# Display new class counts\n",
    "df_upsampled.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=200,     # to match minority class\n",
    "                                 random_state=24) # reproducible results\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "# Display new class counts\n",
    "df_downsampled.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=587)\n",
    "X_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)\n",
    "print(len(y_SMOTE))\n",
    "print(y_SMOTE.sum())\n",
    "print(y_SMOTE.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance using ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf=ExtraTreesClassifier()\n",
    "clf=clf.fit(x,y)\n",
    "\n",
    "scores = pd.DataFrame(clf.feature_importances_,columns=['scores'])\n",
    "\n",
    "features=pd.DataFrame(data.columns,columns=['features'])\n",
    "\n",
    "sc = pd.concat([features,scores],axis=1)\n",
    "\n",
    "for i in sc['scores'].sort_values(ascending=False).head(15).index:\n",
    "    print(sc.loc[i,'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for finding the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_outliers(df,features):\n",
    "    outlier_indices = []\n",
    "    \n",
    "    for c in features:\n",
    "        #1st quartile\n",
    "        Q1 = np.percentile(df[c],25)\n",
    "        #3rd quartile\n",
    "        Q3 = np.percentile(df[c],75)\n",
    "        #IQR\n",
    "        IQR = Q3-Q1\n",
    "        #Outlier Step\n",
    "        outlier_step= IQR * 1.5\n",
    "        #Detect outlier and their indices\n",
    "        outlier_list_col = df[(df[c]<Q1 - outlier_step) | (df[c]> Q3 + outlier_step)].index\n",
    "        #store indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(i for i,v in outlier_indices.items() if v > 1)\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "\n",
    "train_df.loc[detect_outliers(train_df,[\"Age\",\"Fare\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating multiple visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,50))\n",
    "for i in enumerate(cat_col):\n",
    "    plt.subplot(8,1,i[0]+1)\n",
    "    sb.countplot(x=i[1],data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting how every feature co-relate with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "for i, column in enumerate(categorical_col, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    g = sns.barplot(x=f\"{column}\", y='Attrition', data=df)\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "    plt.ylabel('Attrition Count')\n",
    "    plt.xlabel(f'{column}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing missing values using bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "miss_value = pd.DataFrame(train_data.isnull().sum()).reset_index()\n",
    "miss_value.columns=['Columns','No_of_missing_values']\n",
    "miss_value = miss_value[miss_value['No_of_missing_values']>0]\n",
    "sb.barplot(data = miss_value,x='No_of_missing_values',y = 'Columns',orient = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
